# üéØ SAC Pick-and-Place Integration Guide

## üìã **Overview**

This integration combines **SAC (Soft Actor-Critic) reinforcement learning** with **pick-and-place tasks** using **object detection** and **joint state observations**. The system learns to optimize for **accuracy**, **speed**, **energy efficiency**, and **task completion**.

## üéØ **Key Features**

### **Priority-Based Design:**
- **Primary Focus**: Joint state accuracy (proprioception)
- **Secondary Focus**: Object detection (perception)
- **Integration**: State machine for complete pick-and-place sequence

### **Reward Function (Your Specifications):**
- **Speed**: 30% weight
- **Energy Efficiency**: 30% weight  
- **Accuracy**: 20% weight
- **Task Completion**: 20% weight

### **Observation Space (40D):**
- **Joint States (12D)**: 6 joint positions + 6 joint velocities
- **Object Detection (20D)**: 4 objects √ó 5 features (x, y, z, color, confidence)
- **Task Context (8D)**: State machine state, target object, gripper state, progress

## üöÄ **Quick Start**

### **1. Launch the Complete System:**
```bash
cd /home/bouri/roboset/simple_manipulator_ws
source devel/setup.bash
roslaunch pick_and_place sac_training.launch
```

### **2. Test Integration:**
```bash
cd /home/bouri/roboset/simple_manipulator_ws
source devel/setup.bash
python3 src/pick_and_place/scripts/test_sac_integration.py
```

### **3. Start Training:**
```bash
cd /home/bouri/roboset/simple_manipulator_ws
source devel/setup.bash
python3 src/pick_and_place/scripts/sac_pick_place_trainer.py
```

## üìÅ **File Structure**

```
simple_manipulator_ws/src/pick_and_place/
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ pick_place_sac_env.py          # SAC Environment
‚îÇ   ‚îú‚îÄ‚îÄ sac_pick_place_trainer.py      # SAC Training Script
‚îÇ   ‚îú‚îÄ‚îÄ test_sac_integration.py        # Integration Test
‚îÇ   ‚îú‚îÄ‚îÄ perception_module.py           # Object Detection
‚îÇ   ‚îî‚îÄ‚îÄ ros_controller.py              # Robot Control
‚îú‚îÄ‚îÄ launch/
‚îÇ   ‚îî‚îÄ‚îÄ sac_training.launch            # Complete System Launch
‚îî‚îÄ‚îÄ msg/
    ‚îî‚îÄ‚îÄ DetectedObject.msg             # Object Detection Messages
```

## üîß **System Architecture**

### **1. Environment (`pick_place_sac_env.py`)**
```python
class PickPlaceSACEnvironment:
    """
    SAC Environment for Pick-and-Place Tasks
    
    Observation Space (40D):
    - Joint states (12D): positions + velocities
    - Object detection (20D): 4 objects √ó 5 features
    - Task context (8D): state machine + progress
    
    Action Space (6D):
    - Joint torques for 6 main joints
    """
```

### **2. SAC Agent (`sac_pick_place_trainer.py`)**
```python
class SACAgent:
    """
    Soft Actor-Critic agent with:
    - Actor network (policy)
    - Two critic networks (Q-functions)
    - Target networks for stability
    - Experience replay buffer
    """
```

### **3. State Machine Integration**
The system uses the existing pick-and-place state machine:
- **Home**: Initial position
- **Selecting**: Choose target object
- **Picking**: Grasp object
- **Placing**: Place in correct bin

## üìä **Training Process**

### **1. Online Learning:**
- **Real-time**: Continuous learning during simulation
- **Replay Buffer**: Stores experiences for stable learning
- **State Machine**: Guides policy through pick-and-place sequence

### **2. Reward Optimization:**
```python
def calculate_reward(self, action):
    reward = 0.0
    
    # Speed (30% weight)
    speed_reward = self._calculate_speed_reward()
    reward += 0.3 * speed_reward
    
    # Energy (30% weight)
    energy_reward = self._calculate_energy_reward(action)
    reward += 0.3 * energy_reward
    
    # Accuracy (20% weight)
    accuracy_reward = self._calculate_accuracy_reward()
    reward += 0.2 * accuracy_reward
    
    # Completion (20% weight)
    completion_reward = self._calculate_completion_reward()
    reward += 0.2 * completion_reward
    
    return reward
```

### **3. Observation Processing:**
```python
def _get_observation(self):
    # Joint states (12D)
    joint_obs = np.concatenate([
        self.joint_positions,      # 6D
        self.joint_velocities      # 6D
    ])
    
    # Object detection (20D)
    vision_obs = self._encode_objects()  # 4 objects √ó 5 features
    
    # Task context (8D)
    context_obs = np.array([
        self.state_machine_state,  # Current state
        self.target_object_id,     # Target object
        self.gripper_state,        # Gripper state
        self.task_progress,        # Progress
        # ... additional context
    ])
    
    return np.concatenate([joint_obs, vision_obs, context_obs])
```

## üéØ **Key Questions Answered**

### **1. Can we learn the whole pick-and-place sequence?**
**YES!** The integration uses the existing state machine from `@pick-and-place/` to guide the policy through the complete sequence:
- **State Machine**: Provides high-level task structure
- **SAC Policy**: Learns low-level control within each state
- **Integration**: Policy learns to optimize actions within state machine constraints

### **2. How does object detection integrate with joint states?**
**Multi-modal Observation Space:**
- **Joint States (Primary)**: Direct proprioceptive feedback
- **Object Detection (Secondary)**: Visual perception of targets
- **Fusion**: Combined in 40D observation vector for policy learning

### **3. How does the system handle perception failures?**
**Real-world Constraint Approach:**
- **No Fallback**: System must learn to work with available perception
- **Robust Learning**: SAC learns to handle noisy/incomplete observations
- **State Machine**: Provides structure when perception is unreliable

## üìà **Expected Performance**

### **Training Metrics:**
- **Episode Reward**: Should increase over time
- **Task Completion**: Percentage of successful pick-and-place sequences
- **Energy Efficiency**: Torque usage optimization
- **Speed**: Time to complete tasks

### **Success Criteria:**
- **Accuracy**: Precise positioning and grasping
- **Speed**: Fast task completion
- **Energy**: Minimal torque usage
- **Reliability**: Consistent performance across episodes

## üõ†Ô∏è **Customization Options**

### **1. Reward Weights:**
```python
reward_weights = {
    'speed': 0.3,        # 30% weight
    'energy': 0.3,       # 30% weight
    'accuracy': 0.2,     # 20% weight
    'completion': 0.2    # 20% weight
}
```

### **2. Training Parameters:**
```python
trainer = PickPlaceSACTrainer(
    max_episodes=1000,    # Training episodes
    max_steps=1000,       # Steps per episode
    batch_size=256,       # Training batch size
    update_frequency=1,   # Update frequency
    save_frequency=100    # Model save frequency
)
```

### **3. Observation Space:**
- **Joint States**: Can be extended with more joint information
- **Object Detection**: Can include more object features
- **Task Context**: Can add more state machine information

## üîç **Troubleshooting**

### **Common Issues:**

1. **ROS Communication Errors:**
   ```bash
   # Check if ROS is running
   roscore
   
   # Check topics
   rostopic list
   ```

2. **Environment Initialization:**
   ```bash
   # Make sure Gazebo is running
   gazebo
   
   # Check robot spawn
   rostopic echo /franka_state_controller/joint_states
   ```

3. **Perception Module:**
   ```bash
   # Check object detection
   rostopic echo /object_detection
   
   # Check camera topics
   rostopic list | grep camera
   ```

### **Debug Mode:**
```python
# Enable debug logging
import logging
logging.basicConfig(level=logging.DEBUG)
```

## üìö **References**

- **SAC Algorithm**: Soft Actor-Critic for Continuous Control
- **Pick-and-Place**: State machine approach from `@pick-and-place/`
- **ROS Integration**: Gazebo simulation with ROS control
- **Multi-modal Learning**: Combining proprioception and perception

## üéâ **Next Steps**

1. **Test Integration**: Run `test_sac_integration.py`
2. **Start Training**: Run `sac_pick_place_trainer.py`
3. **Monitor Progress**: Check training logs and metrics
4. **Evaluate Performance**: Test trained policy
5. **Optimize**: Adjust parameters based on results

The system is now ready for **online SAC training** with **pick-and-place tasks** using **object detection** and **joint state observations**! üöÄ
